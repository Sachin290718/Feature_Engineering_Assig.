{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8e1a00-2547-41c0-8252-5ca2de3f8cc3",
   "metadata": {},
   "source": [
    "# Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007ddda-156a-43df-af14-9ce3e06baec5",
   "metadata": {},
   "source": [
    "* Filter methods evaluate each feature independently with target variable. Feature with high correlation with target variable are selected as it means this feature has some relation and can help us in making predictions. These methods are used in the preprocessing phase to remove irrelevant or redundant features based on statistical tests (correlation) or other criteria.\n",
    "* Some techniques used are:\n",
    "\n",
    " 1) Information Gain\n",
    " 2) Chi-square test\n",
    " 3) Fisherâ€™s Score\n",
    " 4) Correlation Coefficient\n",
    " 5) Variance Threshold\n",
    " 6) Mean Absolute Difference (MAD)\n",
    " 7) Dispersion Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a36447-56d6-47a5-ab01-65c2786141c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4af67561-86cd-4129-9f88-030eee46096d",
   "metadata": {},
   "source": [
    "# Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8806435-5fe5-4b41-a1e9-e89520504d4b",
   "metadata": {},
   "source": [
    "* Filter methods (e.g. information gain) are based on a statistical analysis of the attributes. Wrapper methods utilize a search algorithm along with a classifier and test the performance of each subset of features.\n",
    "* Dataset size: Filter methods are generally faster for large datasets, while wrapper methods might be suitable for smaller datasets.\n",
    "* Model type: Some models, like tree-based models, have built-in feature selection capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b212eda-d3db-4ae3-9cd7-d65f3454867c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18d549a0-c1e6-4c39-a0f0-6fd52adc5359",
   "metadata": {},
   "source": [
    "# Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca32aaf-8732-4b09-b49d-bcd4a61b7ebd",
   "metadata": {},
   "source": [
    "* Embedded methods perform feature selection during the model training process. They combine the benefits of both filter and wrapper methods. Feature selection is integrated into the model training allowing the model to select the most relevant features based on the training process dynamically.\n",
    "* Some techniques used are:\n",
    "\n",
    "  1) L1 Regularization (Lasso): A regression method that applies L1 regularization to encourage sparsity in the model. Features with non-zero              coefficients are considered important.\n",
    "  2) Decision Trees and Random Forests: These algorithms naturally perform feature selection by selecting the most important features for splitting        nodes based on criteria like Gini impurity or information gain.\n",
    "  3) Gradient Boosting: Like random forests gradient boosting models select important features while building trees by prioritizing features that          reduce error the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fba81a-af53-4bd3-b964-4f6304529a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eea03701-ab1c-4476-b494-b4c2ad8c36ad",
   "metadata": {},
   "source": [
    "# Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384f950-00b6-475d-955a-b3cf12d82c77",
   "metadata": {},
   "source": [
    "* The primary drawback of filter methods for feature selection is that they evaluate features independently, disregarding potential interactions between them.\n",
    "* Ignores Feature Interactions:\n",
    "Filter methods assess each feature's relevance in isolation, without considering how different features might complement or interact with each other. This can lead to the selection of features that are individually weak but become valuable when combined. \n",
    "* Redundant Feature Selection:\n",
    "Due to the lack of consideration for feature interactions, filter methods may select redundant features, which can lead to overfitting and reduced model generalization. \n",
    "* May Miss Optimal Feature Subsets:\n",
    "The independent evaluation of features can prevent the selection of optimal subsets, potentially leading to a model with lower performance than possible with a different feature combination. \n",
    "* Limited Understanding of Model Performance:\n",
    "Filter methods don't take into account the specific model being used, so they might not be ideal for all types of classification or prediction tasks. \n",
    "* Difficulty Choosing the Right Filter:\n",
    "Selecting the appropriate filter method and parameters for a given dataset can be challenging, as there's no universal best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90b15fc-9edc-4320-a61c-4b07dd8515a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d9674fe-57cb-40f4-8b5f-52f85a8b951e",
   "metadata": {},
   "source": [
    "# Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6edbd0-5ee5-4990-b8a3-fa7e127bd740",
   "metadata": {},
   "source": [
    "filter based methods use some mathematical evaluation function (that are based on the intrinsic characteristic of the training set like correlation or recently the Mutual information).\n",
    "however the wrapper methods use a classification perfromance of an classifier (like accuracy ) to do the evaluation.\n",
    "wrapper based are advantageous for giving better performances since they use the  target classifier the feature selection algorithm but they suffer from being computaionnaly expensive.\n",
    "When we compare the filter to the wrapper methods, filter methods are less accurate but faster to compute.\n",
    "so for an online work, i think apply a filter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2b7f0-fcd0-4f99-84a3-b440517b7bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ce58307-fed7-4748-951d-d3f20372acc1",
   "metadata": {},
   "source": [
    "# Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de36f9-2b1d-4758-98bc-82cfb7aa69a6",
   "metadata": {},
   "source": [
    "* To select pertinent attributes using the Filter Method in a telecom churn prediction project, I would:\n",
    "1. Explore the data:\n",
    "I would start by understanding the dataset and its attributes, including their data types and potential relationships to churn.\n",
    "2. Identify potential churn indicators:\n",
    "I would look for features that are known or suspected to be related to churn, such as customer demographics, usage patterns, service quality, and contract details. \n",
    "3. Apply feature selection techniques:\n",
    "I would use metrics like information gain, chi-squared statistic, or correlation analysis to assess the importance of each feature in predicting churn. Libraries like scikit-learn in Python can assist with these calculations. \n",
    "4. Rank and select features:\n",
    "I would rank the features based on their importance and select a subset of the most relevant features for the model.\n",
    "5. Iterate and refine:\n",
    "I would iterate on the feature selection process, evaluating different feature combinations and metrics to find the best performing set of features for the churn prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c69c4-64f5-4dd7-94bd-b411abc5835b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40959326-4bd5-42e9-8bc8-cf8c0c7e3f16",
   "metadata": {},
   "source": [
    "# Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52d9d1-ad41-480e-b73d-f5449a4d2cb7",
   "metadata": {},
   "source": [
    "1. Choose an Appropriate Model:\n",
    "Regularized Models:\n",
    "Lasso Regression (using L1 regularization) is particularly well-suited. It shrinks the coefficients of less important features to zero, effectively performing feature selection during training.\n",
    "Tree-Based Models:\n",
    "Random Forests and Gradient Boosting algorithms also provide feature importance scores, reflecting how much each feature contributes to the model's predictive accuracy. \n",
    "2. Train the Model:\n",
    "Train the chosen model on your soccer match dataset, including both features and the target variable (match outcome).\n",
    "Lasso Regression will automatically penalize the coefficients of irrelevant features, effectively eliminating them.\n",
    "Tree-based models will calculate feature importance scores based on how much each feature contributes to reducing impurity in the decision trees. \n",
    "3. Extract Feature Importance Scores:\n",
    "Lasso Regression:\n",
    "The coefficients of the features are directly extracted after training. Any feature with a coefficient of zero has been effectively eliminated.\n",
    "Tree-Based Models:\n",
    "Access the feature importance scores provided by the model after training. These scores typically indicate how much each feature contributes to reducing the model's prediction error. \n",
    "4. Select Features Based on Importance:\n",
    "Thresholding:\n",
    "Set a threshold for the feature importance scores. Features with scores above the threshold are considered important and are selected.\n",
    "Ranking:\n",
    "If necessary, you can rank the features based on their importance scores and select the top-ranked features. \n",
    "5. Evaluate the Model:\n",
    "Train and test the model using only the selected features.\n",
    "Compare the performance of the model (e.g., accuracy, precision, recall) with the performance of the model using all features. This helps you determine if the feature selection has improved the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b8b93-7471-4340-92da-651895abd421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b07bc51b-4cdd-4072-88ef-b704f477ff3c",
   "metadata": {},
   "source": [
    "# Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d96ff7-08fa-48a2-832e-c62f85de29bd",
   "metadata": {},
   "source": [
    "1. Define the Model:\n",
    "Choose a suitable regression model for house price prediction, such as linear regression, support vector regression, or a more complex model like a decision tree or random forest. \n",
    "2. Define Evaluation Metric:\n",
    "Select a metric to evaluate model performance, like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or R-squared. \n",
    "3. Feature Subset Generation:\n",
    "Start with all features available.\n",
    "Iteratively remove or add features, creating different combinations. \n",
    "For example, you could start with one feature, then add another, and so on, or use more sophisticated methods like forward or backward selection. \n",
    "4. Model Training and Evaluation:\n",
    "For each feature subset, train the chosen model on the training data using the selected features. \n",
    "Evaluate the model's performance on a separate validation set using the chosen metric. \n",
    "5. Feature Set Selection:\n",
    "Compare the performance of different feature subsets based on the chosen evaluation metric. \n",
    "Select the feature subset that yields the best performance. \n",
    "6. Final Model Training and Testing:\n",
    "Train the final model using the selected feature set and the entire dataset (training and validation). \n",
    "Evaluate the final model's performance on a held-out test set to assess its generalization ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3965d-c04d-4cc5-a16c-219be6f1ab76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a943e6-700c-469a-9091-7f2dd56fcefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bf0c0-f988-49d9-8439-9509d38cd009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce875e-3fae-44e1-864a-cf81b84d7629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28eefe5-78b9-43e4-913b-e82169318a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a53b9-8f98-4c86-bd0b-cc67d70b78ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f596a9-a586-404f-8bcb-885d94ec6934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93465f-c549-45a9-a26e-85bee8d3ce84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d5f2e-179c-414d-8cd1-2a92c742baff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78354e5-213a-4579-bc5f-572569324d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2bdb6-c212-4126-b06e-55ba8ae987f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cde18c-7520-4039-ae1f-6a4a124e2dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2344b-cb5e-4aa5-934c-93c5f91a4f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa4c05-301c-4da2-a9fe-e2552af8da78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
